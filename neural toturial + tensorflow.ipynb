{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b025e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import cv2 as cv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b36ff2",
   "metadata": {},
   "source": [
    "**Preprocessing and splitting** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d8ed6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try to load the data \n",
    "data  = keras.datasets.mnist\n",
    "# training section       #testing section\n",
    "(x_train , y_train) , (x_test, y_test) = data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63210326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the x_train shape , y_train shape :  ((60000, 28, 28), (60000,))\n"
     ]
    }
   ],
   "source": [
    "print (f'the x_train shape , y_train shape :  {x_train.shape ,y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0889a27e",
   "metadata": {},
   "source": [
    "**Normalizing pixles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31c45f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the data from 0->255 into 0->1\n",
    "x_train  = x_train/255.0 #do not forget to make it float \n",
    "x_test = x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eab22e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport cv2\\nfor i in range(6):\\n    cv2.imshow(f'Image {i}', x_train[i])  # Opens 6 separate windows\\n    cv2.waitKey(0)  # Press any key to close each window\\ncv2.destroyAllWindows()\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGKCAYAAACsHiO8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAvGElEQVR4nO3de3yU1Z3H8d8EyXBLJnJLyEIkXVmxpYJNSYzyQqyRaCtyW62uClpXLCRUoFoXl5t4iYK1FRpLX3Ul3qGsAoWuFzZAWDWJEmD7opEsWgrRXIDWzIQACWbO/sHLqfGcyEwyOTPP5PN+vZ4/8s1zOU/4gT+fnOeMSymlBAAAwJK4SA8AAAB0LzQfAADAKpoPAABgFc0HAACwiuYDAABYRfMBAACsovkAAABW0XwAAACraD4AAIBVNB8AAMCqLms+CgsLZfjw4dKrVy/JysqS999/v6suBYQVtQunonbhFK6u+GyX9evXy4wZM2TNmjWSlZUlv/zlL2XDhg1SVVUlgwcP/tpj/X6/1NTUSEJCgrhcrnAPDd2EUkoaGxslNTVV4uKC77GpXUQatQunCql2VRfIzMxUeXl5ga9bW1tVamqqKigoOOex1dXVSkTY2MKyVVdXU7tsjtyoXTanbsHUbth/7dLS0iIVFRWSk5MTyOLi4iQnJ0dKS0u1/Zubm8Xn8wU2xYfsIowSEhKC3pfaRTShduFUwdRu2JuP48ePS2trqyQnJ7fJk5OTpa6uTtu/oKBAPB5PYEtLSwv3kNCNhfIImdpFNKF24VTB1G7E33ZZuHCheL3ewFZdXR3pIQFBoXbhVNQuIu28cJ9w4MCB0qNHD6mvr2+T19fXS0pKira/2+0Wt9sd7mEAIaN24VTULpwm7E8+4uPjJSMjQ4qLiwOZ3++X4uJiyc7ODvflgLChduFU1C4cJ6Tp1EFat26dcrvdqqioSFVWVqpZs2appKQkVVdXd85jvV5vxGfqssXO5vV6qV02R27ULptTt2Bqt0uaD6WUWr16tUpLS1Px8fEqMzNTlZWVBXUcfwnYwrmF+g84tcsWLRu1y+bULZja7ZJFxjrD5/OJx+OJ9DAQI7xeryQmJlq5FrWLcKJ24VTB1G7E33YBAADdC80HAACwiuYDAABYRfMBAACsovkAAABW0XwAAACraD4AAIBVNB8AAMAqmg8AAGAVzQcAALCK5gMAAFhF8wEAAKw6L9IDAICMjAwty8/PN+47Y8YMLXvhhRe0bPXq1Vq2Z8+eDowOQLjx5AMAAFhF8wEAAKyi+QAAAFbRfAAAAKuYcBplevTooWUej6fD52tv0l6fPn207KKLLtKyvLw8LXvyySe17JZbbjFe5/Tp01r2+OOPa9lDDz1kPB6xZ8yYMVq2bds2LUtMTDQer5TSsttvv13LbrjhBi0bMGBAECMEos/VV1+tZS+//LJx3yuvvFLLqqqqwj6mzuDJBwAAsIrmAwAAWEXzAQAArKL5AAAAVjHhtBPS0tK0LD4+Xssuv/xyLRs3bpzxnElJSVo2ffr00AfXAZ988omWrVq1SsumTp2qZY2NjcZz/u///q+WlZSUdGB0cKLMzEwte+2117TMNKnaNLFUxFxrLS0tWmaaXHrZZZdpWXurnprOidCMHz9ey0x/Lhs3brQxHEcbO3asln3wwQcRGEl48OQDAABYRfMBAACsovkAAABW0XwAAACrmHAaBNOKjCIi27dv17LOrEZqk9/v17JFixZp2YkTJ7TMtKpebW2t8TqfffaZlkXbSnsIjWl1XBGR73znO1r20ksvadmQIUM6df2DBw9q2YoVK7Rs3bp1Wvbuu+9qmanuRUQKCgo6MDp82YQJE7RsxIgRWsaE07bi4vTnAunp6Vp2wQUXGI93uVxhH1O48eQDAABYRfMBAACsovkAAABW0XwAAACraD4AAIBVvO0ShCNHjhjzv/71r1pm622X8vJyLWtoaNCyq666yni8aenoF198sdPjQuz7zW9+Y8xvueUWK9c3vVXTr18/LTMt4296++KSSy4Jy7igmzFjhpaVlpZGYCTOYnoj7O6779Yy09tkIiIHDhwI+5jCjScfAADAKpoPAABgFc0HAACwiuYDAABYxYTTIPztb38z5vfff7+WXX/99Vq2d+9eLVu1alXQ19+3b5+WXXPNNVrW1NSkZd/61reM57z33nuDvj66r4yMDC37wQ9+YNw32CWdTRNBt2zZomVPPvmk8fiamhotM/0dMy3t/73vfU/LnLAUtVOZlgnHuT377LNB7Wf6qAGnoDIAAIBVNB8AAMCqkJuPXbt2yaRJkyQ1NVVcLpds2rSpzfeVUrJkyRIZMmSI9O7dW3Jychz9aAixg9qFU1G7iDUhNx9NTU0yevRoKSwsNH5/xYoVsmrVKlmzZo2Ul5dL3759JTc3V06fPt3pwQKdQe3CqahdxBqXUkp1+GCXSzZu3ChTpkwRkbPdd2pqqvz0pz+V++67T0REvF6vJCcnS1FRkdx8883nPKfP57O2SmhXSExM1LLGxkYta2+VyLvuukvLbrvtNi179dVXOzC67sfr9Rr/TKhd3ZgxY7Rs+/btWmb6ebbnjTfe0DLTSqhXXnmllrW38qhpMt6xY8eCGk9ra6uWnTx50rivaUx79uwJ6jrh4LTaNf15mVYzff3117Xs9ttv79S1Y817772nZZdddpmWXX755cbjy8rKwj6mULRXu18W1jkfhw4dkrq6OsnJyQlkHo9HsrKyWFIXUY3ahVNRu3CisL5qW1dXJyIiycnJbfLk5OTA976qublZmpubA1/7fL5wDgkICrULp6J24UQRf9uloKBAPB5PYBs2bFikhwQEhdqFU1G7iLSwNh8pKSkiIlJfX98mr6+vD3zvqxYuXCherzewVVdXh3NIQFCoXTgVtQsnCuuvXdLT0yUlJUWKi4sDk9d8Pp+Ul5fL7Nmzjce43W5xu93hHEZEBfv40uv1Bn1O00cpr1+/Xsv8fn/Q50Rb3a12/+mf/knLTCv2miYhHj9+3HjO2tpaLXv++ee17MSJE1r2hz/8IaisK/Tu3duY//SnP9WyW2+9tauHE7Joqd3vf//7WtbezxZ/99Vfl4mc/TMNxqeffhru4VgTcvNx4sQJ+eijjwJfHzp0SPbt2yf9+/eXtLQ0mTdvnjzyyCMyYsQISU9Pl8WLF0tqampgZjYQKdQunIraRawJufnYvXu3XHXVVYGvFyxYICIiM2fOlKKiIvnZz34mTU1NMmvWLGloaJBx48bJm2++Kb169QrfqIEOoHbhVNQuYk3IzceECRPk65YGcblcsnz5clm+fHmnBgaEG7ULp6J2EWsi/rYLAADoXmg+AACAVWF92wXBW7ZsmTHPyMjQMtMyz19ezfALb7/9dqfHhdjS3hsNTz75pJaZ3lYwfTTAjBkzjOfcvXu3ljn5bYe0tLRID8FRLrrooqD2+9Of/tTFI3EW099F0xsw//d//6dlpr+fTsGTDwAAYBXNBwAAsIrmAwAAWEXzAQAArGLCaYQ0NTUZc9NS6nv27NGy3/72t1q2Y8cOLTNNAhQRKSws1LKvW0cAznTppZcac9PkUpPJkydrWUlJSafGhO7tgw8+iPQQwioxMVHLrr32WuO+t912m5ZNnDgxqOs8/PDDWtbQ0BDUsdGIJx8AAMAqmg8AAGAVzQcAALCK5gMAAFjFhNMo8/HHH2vZHXfcoWVr167Vsttvvz2oTESkb9++WvbCCy9oWW1trfF4OMNTTz1lzF0ul5aZJpLG2uTSuDj9/7f8fn8ERtJ99e/fP+znHD16tJaZaty0MrSIyNChQ7UsPj5ey2699VYtM9XUqVOnjNcpLy/XsubmZi077zz9P80VFRXGczoVTz4AAIBVNB8AAMAqmg8AAGAVzQcAALCKCacOsHHjRi07ePCglpkmF1599dXGcz722GNadsEFF2jZo48+qmWffvqp8ZyIrOuvv17LxowZY9zXtJrt73//+3APKeqYJpe2t7Lvvn37ung0scU0ydL0s12zZo2WPfjgg5269iWXXKJlpgmnn3/+ufH4kydPalllZaWWPffcc1pmWkW6vYna9fX1WvbJJ59oWe/evbXswIEDxnM6FU8+AACAVTQfAADAKpoPAABgFc0HAACwigmnDrV//34tu+mmm7Rs0qRJxuNNK6Tec889WjZixAgtu+aaa4IZIiwzTVIzrdIoInL06FEtW79+fdjHZIvb7dayZcuWBXXs9u3bjfnChQs7M6RuZ86cOVp2+PBhLbv88svDfu0jR45o2aZNm7Tsww8/NB5fVlYW7iEZzZo1S8sGDRqkZX/+859tDCeiePIBAACsovkAAABW0XwAAACraD4AAIBVNB8AAMAq3naJIQ0NDVr24osvGvd99tlntey88/RyGD9+vJZNmDBBy3bu3HnO8SF6NDc3a1ltbW0ERhI605stixYt0rL7779fy0xLWf/85z83XufEiRMdGB2+7Iknnoj0EKJKex938VWvvfZaF48k8njyAQAArKL5AAAAVtF8AAAAq2g+AACAVUw4dahLLrlEy/75n/9Zy8aOHWs83jS51KSyslLLdu3aFdSxiF6///3vIz2EcxozZowxN00k/eEPf6hlmzdv1rLp06d3elxAV9u4cWOkh9DlePIBAACsovkAAABW0XwAAACraD4AAIBVTDiNMhdddJGW5efna9m0adO0LCUlpVPXbm1t1TLTqpd+v79T10HXcLlcQWUiIlOmTNGye++9N9xDCtr8+fO1bPHixcZ9PR6Plr388staNmPGjM4PDECX4MkHAACwiuYDAABYFVLzUVBQIGPHjpWEhAQZPHiwTJkyRaqqqtrsc/r0acnLy5MBAwZIv379ZPr06VJfXx/WQQOhonbhVNQuYlFIzUdJSYnk5eVJWVmZbNu2Tc6cOSMTJ06UpqamwD7z58+XLVu2yIYNG6SkpERqamqM8xMAm6hdOBW1i1jkUkqpjh587NgxGTx4sJSUlMj48ePF6/XKoEGD5JVXXgmstnngwAG5+OKLpbS0VC677LJzntPn8xknlDmZaSLoLbfcYtzXNLl0+PDh4R6S7N69W8seffRRLXPCSphfx+v1SmJiopbHYu3eeOONWvbqq68a9zVNLv7Nb36jZc8995yW/fWvfzWe0/Qzuv3227Vs9OjRWjZ06FAtO3LkiPE6ZWVlWvb0008HtZ+TdKfa7S7Wr1+vZTfddJOWzZw5U8teeOGFLhlTV2ivdr+sU3M+vF6viIj0799fREQqKirkzJkzkpOTE9hn5MiRkpaWJqWlpZ25FBBW1C6citpFLOjwq7Z+v1/mzZsnV1xxhYwaNUpEROrq6iQ+Pl6SkpLa7JucnCx1dXXG8zQ3N0tzc3Pga5/P19EhAUGhduFU1C5iRYeffOTl5cn+/ftl3bp1nRpAQUGBeDyewDZs2LBOnQ84F2oXTkXtIlZ0qPnIz8+XrVu3yo4dO9r8rjYlJUVaWlqkoaGhzf719fXtLoC1cOFC8Xq9ga26urojQwKCQu3CqahdxJKQfu2ilJK5c+fKxo0bZefOnZKent7m+xkZGdKzZ08pLi4OfHR1VVWVHDlyRLKzs43ndLvd4na7Ozj8yEpOTtayb37zm1r2q1/9SstGjhwZ9vGUl5dr2cqVK437mj5uPJZXLqV22+rRo4eWzZkzR8tMH0Hf3iP6ESNGdHg87733npbt2LHDuO+SJUs6fB0nonZjm+mdj7i42F+CK6TmIy8vT1555RXZvHmzJCQkBH6f6PF4pHfv3uLxeOSuu+6SBQsWSP/+/SUxMVHmzp0r2dnZQc24BroKtQunonYRi0JqPn7961+LiMiECRPa5GvXrpU77rhDRER+8YtfSFxcnEyfPl2am5slNzdXnnnmmbAMFugoahdORe0iFoX8a5dz6dWrlxQWFkphYWGHBwWEG7ULp6J2EYti/xdLAAAgqtB8AAAAqzq8yFis+mLVwC8zLTstIjJmzBgt+8Y3vhHuIRnfBPj5z3+uZW+99ZaWnTp1KuzjQXQyrWb5wQcfGPcdO3ZsUOc0vappesurPaal2E1rVNx7771BnxOIdaa3lIqKiuwPpAvx5AMAAFhF8wEAAKyi+QAAAFbRfAAAAKu6zYTTrKwsLbv//vu1LDMzU8v+4R/+IezjOXnypDFftWqVlj322GNa1tTUFPYxwdk++eQTLZs2bZpx33vuuUfLFi1a1KnrP/3001r2xQJZX/bRRx916jpALHG5XJEeQkTw5AMAAFhF8wEAAKyi+QAAAFbRfAAAAKu6zYTTqVOnBpWForKyUsu2bt2qZZ9//rmWmVYoFRFpaGjo1JiAL6utrTXmy5YtCyoDED5vvPGGlt14440RGEnk8eQDAABYRfMBAACsovkAAABW0XwAAACrXEopFelBfJnP5xOPxxPpYSBGeL1eSUxMtHItahfhRO3CqYKpXZ58AAAAq2g+AACAVTQfAADAKpoPAABgFc0HAACwiuYDAABYRfMBAACsovkAAABW0XwAAACraD4AAIBVNB8AAMAqmg8AAGAVzQcAALCK5gMAAFgVdc2HUirSQ0AMsVlP1C7CidqFUwVTT1HXfDQ2NkZ6CIghNuuJ2kU4UbtwqmDqyaWirOX1+/1SU1MjCQkJ0tjYKMOGDZPq6mpJTEyM9NA6zefzcT+WKKWksbFRUlNTJS7OTo9N7TpHNN8PtRte0fxn3RHRfD+h1O55lsYUtLi4OBk6dKiIiLhcLhERSUxMjLofcmdwP3Z4PB6r16N2nSda74faDT/ux45gazfqfu0CAABiG80HAACwKqqbD7fbLUuXLhW32x3poYQF99N9xNrPhvvpPmLtZ8P9RKeom3AKAABiW1Q/+QAAALGH5gMAAFhF8wEAAKyK2uajsLBQhg8fLr169ZKsrCx5//33Iz2koO3atUsmTZokqamp4nK5ZNOmTW2+r5SSJUuWyJAhQ6R3796Sk5MjBw8ejMxgz6GgoEDGjh0rCQkJMnjwYJkyZYpUVVW12ef06dOSl5cnAwYMkH79+sn06dOlvr4+QiOODk6tX2qX2qV2o0Os129UNh/r16+XBQsWyNKlS2XPnj0yevRoyc3NlaNHj0Z6aEFpamqS0aNHS2FhofH7K1askFWrVsmaNWukvLxc+vbtK7m5uXL69GnLIz23kpISycvLk7KyMtm2bZucOXNGJk6cKE1NTYF95s+fL1u2bJENGzZISUmJ1NTUyLRp0yI46shycv1Su9QutRsdYr5+VRTKzMxUeXl5ga9bW1tVamqqKigoiOCoOkZE1MaNGwNf+/1+lZKSolauXBnIGhoalNvtVq+++moERhiao0ePKhFRJSUlSqmzY+/Zs6fasGFDYJ8PP/xQiYgqLS2N1DAjKlbql9rtfqjd6BVr9Rt1Tz5aWlqkoqJCcnJyAllcXJzk5ORIaWlpBEcWHocOHZK6uro29+fxeCQrK8sR9+f1ekVEpH///iIiUlFRIWfOnGlzPyNHjpS0tDRH3E+4xXL9UruxjdqNbrFWv1HXfBw/flxaW1slOTm5TZ6cnCx1dXURGlX4fHEPTrw/v98v8+bNkyuuuEJGjRolImfvJz4+XpKSktrs64T76QqxXL/UbmyjdqNXLNZv1H2wHKJXXl6e7N+/X955551IDwUICbULJ4vF+o26Jx8DBw6UHj16aDN26+vrJSUlJUKjCp8v7sFp95efny9bt26VHTt2BD79UuTs/bS0tEhDQ0Ob/aP9frpKLNcvtRvbqN3oFKv1G3XNR3x8vGRkZEhxcXEg8/v9UlxcLNnZ2REcWXikp6dLSkpKm/vz+XxSXl4elfenlJL8/HzZuHGjbN++XdLT09t8PyMjQ3r27NnmfqqqquTIkSNReT9dLZbrl9qNbdRudIn5+o3whFejdevWKbfbrYqKilRlZaWaNWuWSkpKUnV1dZEeWlAaGxvV3r171d69e5WIqKeeekrt3btXHT58WCml1OOPP66SkpLU5s2b1R//+Ec1efJklZ6erk6dOhXhketmz56tPB6P2rlzp6qtrQ1sJ0+eDOzz4x//WKWlpant27er3bt3q+zsbJWdnR3BUUeWk+uX2qV2qd3oEOv1G5XNh1JKrV69WqWlpan4+HiVmZmpysrKIj2koO3YsUOJiLbNnDlTKXX2ta/Fixer5ORk5Xa71dVXX62qqqoiO+h2mO5DRNTatWsD+5w6dUrNmTNHnX/++apPnz5q6tSpqra2NnKDjgJOrV9ql9qldqNDrNcvn2oLAACsiro5HwAAILbRfAAAAKtoPgAAgFU0HwAAwCqaDwAAYBXNBwAAsIrmAwAAWEXzAQAArKL5AAAAVtF8AAAAq2g+AACAVTQfAADAKpoPAABgFc0HAACwiuYDAABYRfMBAACsovkAAABW0XwAAACraD4AAIBVNB8AAMAqmg8AAGAVzQcAALCK5gMAAFhF8wEAAKyi+QAAAFbRfAAAAKtoPgAAgFU0HwAAwCqaDwAAYBXNBwAAsIrmAwAAWEXzAQAArKL5AAAAVp3XVScuLCyUlStXSl1dnYwePVpWr14tmZmZ5zzO7/dLTU2NJCQkiMvl6qrhIcYppaSxsVFSU1MlLi60HpvaRSRRu3CqkGpXdYF169ap+Ph49dxzz6k//elP6u6771ZJSUmqvr7+nMdWV1crEWFjC8tWXV1N7bI5cqN22Zy6BVO7XdJ8ZGZmqry8vMDXra2tKjU1VRUUFJzz2IaGhoj/4NhiZ2toaKB22Ry5UbtsTt2Cqd2wz/loaWmRiooKycnJCWRxcXGSk5MjpaWl2v7Nzc3i8/kCW2NjY7iHhG4slEfI1C6iCbULpwqmdsPefBw/flxaW1slOTm5TZ6cnCx1dXXa/gUFBeLxeALbsGHDwj0kICjULpyK2oXTRPxtl4ULF4rX6w1s1dXVkR4SEBRqF05F7SLSwv62y8CBA6VHjx5SX1/fJq+vr5eUlBRtf7fbLW63O9zDAEJG7cKpqF04TdiffMTHx0tGRoYUFxcHMr/fL8XFxZKdnR3uywFhQ+3CqahdOE5I06mDtG7dOuV2u1VRUZGqrKxUs2bNUklJSaquru6cx3q93ojP1GWLnc3r9VK7bI7cqF02p27B1G6XNB9KKbV69WqVlpam4uPjVWZmpiorKwvqOP4SsIVzC/UfcGqXLVo2apfNqVswtetSSimJIj6fTzweT6SHgRjh9XolMTHRyrWoXYQTtQunCqZ2I/62CwAA6F5oPgAAgFU0HwAAwCqaDwAAYBXNBwAAsIrmAwAAWEXzAQAArKL5AAAAVtF8AAAAq2g+AACAVTQfAADAKpoPAABgFc0HAACwiuYDAABYRfMBAACsovkAAABWnRfpASB6LFq0SMseeughLYuL03vWCRMmGM9ZUlLS6XEBQLRKSEjQsn79+hn3/cEPfqBlgwYN0rKnnnpKy5qbmzswuujFkw8AAGAVzQcAALCK5gMAAFhF8wEAAKxiwmk3dccdd2jZAw88oGV+vz+o8ymlOjskAIgaw4cP1zLTv5HZ2dlaNmrUqE5de8iQIVr2k5/8pFPnjDY8+QAAAFbRfAAAAKtoPgAAgFU0HwAAwComnHZTF1xwgZb16tUrAiNBrMjKytKy2267TcuuvPJKLfvWt74V9HXuu+8+LaupqdGycePGadlLL72kZeXl5UFfG842cuRIYz5v3jwtu/XWW7Wsd+/eWuZyubSsurraeJ3GxkYtu/jii7Xspptu0rJnnnlGyw4cOGC8jhPw5AMAAFhF8wEAAKyi+QAAAFbRfAAAAKtoPgAAgFW87RLjcnJyjPncuXODOt40m/r666/Xsvr6+tAGBsf64Q9/aMyffvppLRs4cKCWmd4O2Llzp5YNGjTIeJ2VK1eeY4TtX8d0zptvvjmo8yF6eTweLXviiSe0rL3aTUhI6PC1Dx48qGW5ubnGfXv27Kllpn9jTX9vTJmT8eQDAABYRfMBAACsovkAAABW0XwAAACrmHAaQ0zLSa9du9a4r2mClolpct/hw4dDGxgc4bzz9H8Ovvvd72rZb3/7W+Pxffr00bJdu3Zp2cMPP6xl77zzjpa53W7jdX73u99p2cSJE437ftXu3buD2g/OMnXqVC3713/917Bf5+OPP9aya665RsvaW179wgsvDPuYnIonHwAAwCqaDwAAYBXNBwAAsCrk5mPXrl0yadIkSU1NFZfLJZs2bWrzfaWULFmyRIYMGSK9e/eWnJwc4yIsgG3ULpyK2kWsCXnCaVNTk4wePVp+9KMfybRp07Tvr1ixQlatWiXPP/+8pKeny+LFiyU3N1cqKyulV69eYRk0zGbOnKllqampQR9vWmXyhRde6MyQogq1+/Vuu+02LXv22WeDPn7btm1aZlpR0ufzBXW+9lajDHZy6SeffKJlzz//fFDHRhtq9+vdeOONnTr+L3/5i5Z98MEHWvbAAw9oWXuTS00uvvjikMYVy0JuPq677jq57rrrjN9TSskvf/lLWbRokUyePFlEzv7HKzk5WTZt2sQyxogoahdORe0i1oR1zsehQ4ekrq6uzeeJeDweycrKktLSUuMxzc3N4vP52myAbdQunIrahROFtfmoq6sTEZHk5OQ2eXJycuB7X1VQUCAejyewDRs2LJxDAoJC7cKpqF04UcTfdlm4cKF4vd7AFsrvz4BIonbhVNQuIi2sK5ympKSIyNmPVx8yZEggr6+vlzFjxhiPcbvd7a5kiPaZPl75Rz/6kZb5/X7j8Q0NDVr2yCOPdHpcTtXdate0yuiDDz6oZUopLXvmmWeM51y0aJGWdeZx/r//+793+FgRkZ/85CdaduzYsU6dMxp1t9o1ufvuu7Vs1qxZWvb2228bj//oo4+07OjRo50f2Fd89elUdxbWJx/p6emSkpIixcXFgczn80l5eblkZ2eH81JAWFG7cCpqF04U8pOPEydOtOkSDx06JPv27ZP+/ftLWlqazJs3Tx555BEZMWJE4JWv1NRUmTJlSjjHDYSM2oVTUbuINSE3H7t375arrroq8PWCBQtE5OwaE0VFRfKzn/1MmpqaZNasWdLQ0CDjxo2TN998s1u8a47oRu3CqahdxJqQm48JEyYYfw/8BZfLJcuXL5fly5d3amBAuFG7cCpqF7Em4m+7AACA7iWsb7ugawwfPlzLXnvttU6dc/Xq1Vq2Y8eOTp0T0WfJkiXG3PRmS0tLi5a99dZbWmZaYlpE5NSpU0GNyfSrANOS6WlpacbjXS6Xlpne1Nq8eXNQ44Hz1dTUaNmyZcvsD+QcmAD8dzz5AAAAVtF8AAAAq2g+AACAVTQfAADAKiacOsC1116rZZdccklQx3551cMve/rppzs1JkSfpKQkLZszZ45xX9Nrm6bJpZ1dpOrCCy/UspdfflnLMjIygj7nf/7nf2rZihUrQhsYcA6m5fn79u3bqXN++9vfDmq/9957T8va+4Rip+LJBwAAsIrmAwAAWEXzAQAArKL5AAAAVjHhNMqYJvg9/vjjQR37zjvvaNnMmTON+3q93pDGhegXHx+vZQMHDgz6eNMEu8GDB2vZnXfeaTz+hhtu0LJRo0ZpWb9+/bTMNAG2vc8yeemll7SsqanJuC+6rz59+mjZN7/5TeO+S5cu1bLvf//7QV0nLk7/f3i/3x/UsSLm1VlNf8daW1uDPqcT8OQDAABYRfMBAACsovkAAABW0XwAAACrmHAaIcOHDzfmr732WofP+ec//1nL6uvrO3w+OEtLS4uWHTt2zLjvoEGDtOzQoUNa1t6kz2CZJtP5fD4tGzJkiJYdP37ceM4tW7Z0akxwtp49e2rZpZdeqmWmf0tNdSYicurUKS0z1a5plVHTCtSmya7tOe88/T/D06ZN0zLTqtSmv/NOwZMPAABgFc0HAACwiuYDAABYRfMBAACsYsJphDzwwAPGPJSV8b4q2JVQEZsaGhq0zLRirojI1q1btax///5a9vHHH2vZ5s2bjecsKirSsr/97W9atm7dOi0zTQQ07Yfuw7Rir4h5gufrr78e1DkfeughY759+3Yte/fdd7XM9HfEdKxpZd/2mCZ/FxQUaNmRI0e0bNOmTcZzNjc3B339SOHJBwAAsIrmAwAAWEXzAQAArKL5AAAAVtF8AAAAq3jbxYIxY8Zo2cSJEzt1TtMbB1VVVZ06J2JPeXm5MTfNsO8K48eP17Irr7xSy0xveZk+LgCxybRkentvptx///1BnfONN97QstWrVxv3Nb0pZvo78l//9V9a9u1vf1vL2lv2fMWKFVpmejNm8uTJWvbyyy9r2X//938br/PEE09o2WeffWbc96v27dsX1H6dxZMPAABgFc0HAACwiuYDAABYRfMBAACsYsKpBW+//baWnX/++UEfX1ZWpmV33HFHZ4YEWNG7d28tM00uVUppGcurx6YePXpo2cMPP6xl9913n/H4pqYmLfu3f/s3LTPVj2liqYjId7/7XS371a9+pWWXXnqplh08eFDLZs+ebbzOjh07tCwxMVHLLr/8ci279dZbteyGG24wXmfbtm3G/Kuqq6u1LD09PahjO4snHwAAwCqaDwAAYBXNBwAAsIrmAwAAWMWEUwsGDBigZaZJd+155plntOzEiROdGhNgw1tvvRXpISDKzJo1S8tMk0tPnjxpPP6ee+7RMtOk/ssuu0zL7rzzTuM5r7vuOi0zTZZevny5lq1du1bLTBM52+Pz+bTszTffDCq75ZZbjOf8l3/5l6CuPX/+/KD26wo8+QAAAFbRfAAAAKtoPgAAgFUhNR8FBQUyduxYSUhIkMGDB8uUKVO0T1I9ffq05OXlyYABA6Rfv34yffp0qa+vD+uggVBRu3AqahexyKVMSwu249prr5Wbb75Zxo4dK59//rk8+OCDsn//fqmsrJS+ffuKyNmV3f7whz9IUVGReDweyc/Pl7i4OHn33XeDuobP5xOPx9Oxu4kCpslHptVIQ5lw+o1vfEPLDh8+HNK4uiuv1yuJiYnUboTk5uZqmeljyU3/DA0ZMsR4zmPHjnV+YA4Qq7VbW1urZaaPr29ubjYef+DAAS374ufwZRdeeGEHRvd3y5Yt07KCggIta21t7dR1YtEXtft1Qnrb5auzbYuKimTw4MFSUVEh48ePF6/XK//xH/8hr7zyinzve98TkbP/Mb744oulrKzMOPsYsIHahVNRu4hFnZrz4fV6RUSkf//+IiJSUVEhZ86ckZycnMA+I0eOlLS0NCktLTWeo7m5WXw+X5sN6GrULpyK2kUs6HDz4ff7Zd68eXLFFVfIqFGjRESkrq5O4uPjJSkpqc2+ycnJUldXZzxPQUGBeDyewDZs2LCODgkICrULp6J2ESs63Hzk5eXJ/v37O/3JkwsXLhSv1xvYQlmcBegIahdORe0iVnRohdP8/HzZunWr7Nq1S4YOHRrIU1JSpKWlRRoaGtp04fX19ZKSkmI8l9vtFrfb3ZFhRNyYMWO07MuPPr9gmlza0tJiPGdhYaGWMWs9fKhdu0yTpdExsVK7pqcxpgmn7Y1v9OjRQV3HNLF5165dxn03bdqkZX/5y1+0jMml4RPSkw+llOTn58vGjRtl+/btkp6e3ub7GRkZ0rNnTykuLg5kVVVVcuTIEcnOzg7PiIEOoHbhVNQuYlFITz7y8vLklVdekc2bN0tCQkKgg/V4PNK7d2/xeDxy1113yYIFC6R///6SmJgoc+fOlezsbGZcI6KoXTgVtYtYFFLz8etf/1pERCZMmNAmX7t2bWAti1/84hcSFxcn06dPl+bmZsnNzTV+MBpgE7ULp6J2EYtCaj6CWY+sV69eUlhYaJy7AEQKtQunonYRi/hsFwAAYFWH3nbBWV99r15E2p1d/lWffvqpMb/vvvs6MyQgqvzP//yPlsXF6f/PE8rHDcDZxo8fr2VTpkzRsu985zvG448ePaplzz33nJZ99tlnWtbeW4awjycfAADAKpoPAABgFc0HAACwiuYDAABYxYRTAF1m//79Wnbw4EEtMy3D/o//+I/Gcx47dqzzA0PENDY2atmLL74YVIbYwZMPAABgFc0HAACwiuYDAABYRfMBAACsYsJpJxw4cEDL3nvvPS0bN26cjeEAjvDYY49p2bPPPqtljz76qPH4uXPnalllZWXnBwbAGp58AAAAq2g+AACAVTQfAADAKpoPAABglUsppSI9iC/z+Xzi8XgiPQzECK/XK4mJiVauRe0Gx/Tn8bvf/U7LcnJyjMe//vrrWnbnnXdqWVNTUwdGFz2oXThVMLXLkw8AAGAVzQcAALCK5gMAAFhF8wEAAKyi+QAAAFaxvDoAq3w+n5bddNNNWtbe8uqzZ8/WsmXLlmkZS64D0YsnHwAAwCqaDwAAYBXNBwAAsIrmAwAAWMXy6ohpLFENp6J24VQsrw4AAKIOzQcAALCK5gMAAFgVdc1HlE1BgcPZrCdqF+FE7cKpgqmnqGs+GhsbIz0ExBCb9UTtIpyoXThVMPUUdW+7+P1+qampkYSEBGlsbJRhw4ZJdXW1tVnfXcnn83E/liilpLGxUVJTUyUuzk6PTe06RzTfD7UbXtH8Z90R0Xw/odRu1H22S1xcnAwdOlRERFwul4iIJCYmRt0PuTO4HztsvzpI7TpPtN4PtRt+3I8dwdZu1P3aBQAAxDaaDwAAYFVUNx9ut1uWLl0qbrc70kMJC+6n+4i1nw33033E2s+G+4lOUTfhFAAAxLaofvIBAABiD80HAACwiuYDAABYRfMBAACsitrmo7CwUIYPHy69evWSrKwsef/99yM9pKDt2rVLJk2aJKmpqeJyuWTTpk1tvq+UkiVLlsiQIUOkd+/ekpOTIwcPHozMYM+hoKBAxo4dKwkJCTJ48GCZMmWKVFVVtdnn9OnTkpeXJwMGDJB+/frJ9OnTpb6+PkIjjg5OrV9ql9qldqNDrNdvVDYf69evlwULFsjSpUtlz549Mnr0aMnNzZWjR49GemhBaWpqktGjR0thYaHx+ytWrJBVq1bJmjVrpLy8XPr27Su5ubly+vRpyyM9t5KSEsnLy5OysjLZtm2bnDlzRiZOnChNTU2BfebPny9btmyRDRs2SElJidTU1Mi0adMiOOrIcnL9UrvULrUbHWK+flUUyszMVHl5eYGvW1tbVWpqqiooKIjgqDpGRNTGjRsDX/v9fpWSkqJWrlwZyBoaGpTb7VavvvpqBEYYmqNHjyoRUSUlJUqps2Pv2bOn2rBhQ2CfDz/8UImIKi0tjdQwIypW6pfa7X6o3egVa/UbdU8+WlpapKKiQnJycgJZXFyc5OTkSGlpaQRHFh6HDh2Surq6Nvfn8XgkKyvLEffn9XpFRKR///4iIlJRUSFnzpxpcz8jR46UtLQ0R9xPuMVy/VK7sY3ajW6xVr9R13wcP35cWltbJTk5uU2enJwsdXV1ERpV+HxxD068P7/fL/PmzZMrrrhCRo0aJSJn7yc+Pl6SkpLa7OuE++kKsVy/1G5so3ajVyzWb9R9qi2iV15enuzfv1/eeeedSA8FCAm1CyeLxfqNuicfAwcOlB49emgzduvr6yUlJSVCowqfL+7BafeXn58vW7dulR07dgQ+elvk7P20tLRIQ0NDm/2j/X66SizXL7Ub26jd6BSr9Rt1zUd8fLxkZGRIcXFxIPP7/VJcXCzZ2dkRHFl4pKenS0pKSpv78/l8Ul5eHpX3p5SS/Px82bhxo2zfvl3S09PbfD8jI0N69uzZ5n6qqqrkyJEjUXk/XS2W65fajW3UbnSJ+fqN8IRXo3Xr1im3262KiopUZWWlmjVrlkpKSlJ1dXWRHlpQGhsb1d69e9XevXuViKinnnpK7d27Vx0+fFgppdTjjz+ukpKS1ObNm9Uf//hHNXnyZJWenq5OnToV4ZHrZs+erTwej9q5c6eqra0NbCdPngzs8+Mf/1ilpaWp7du3q927d6vs7GyVnZ0dwVFHlpPrl9qldqnd6BDr9RuVzYdSSq1evVqlpaWp+Ph4lZmZqcrKyiI9pKDt2LFDiYi2zZw5Uyl19rWvxYsXq+TkZOV2u9XVV1+tqqqqIjvodpjuQ0TU2rVrA/ucOnVKzZkzR51//vmqT58+aurUqaq2tjZyg44CTq1fapfapXajQ6zXr0sppbr22QoAAMDfRd2cDwAAENtoPgAAgFU0HwAAwCqaDwAAYBXNBwAAsIrmAwAAWEXzAQAArKL5AAAAVtF8AAAAq2g+AACAVTQfAADAKpoPAABg1f8DHxVOitQDdo8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range (6):\n",
    "  plt.subplot(2 , 3 , i+1)\n",
    "  plt.imshow(x_train[i] ,cmap='grey')\n",
    "#plt.show() can be put or removed only in jupyter but in python script it is a must \n",
    "\n",
    "#why can't we use open cv ? cv2.imshow() ? it will shows the result one after another \n",
    "#and w a wait key between images so it will be like this and there is no a subplots in openCV\n",
    "'''\n",
    "import cv2\n",
    "for i in range(6):\n",
    "    cv2.imshow(f'Image {i}', x_train[i])  # Opens 6 separate windows\n",
    "    cv2.waitKey(0)  # Press any key to close each window\n",
    "cv2.destroyAllWindows()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4dd581",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91a13551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moham\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m100,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,770</span> (397.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m101,770\u001b[0m (397.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,770</span> (397.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m101,770\u001b[0m (397.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#pass a list with all the diff layers \n",
    "#.dense means = fully connected layer \n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = (28 ,28)),\n",
    "    keras.layers.Dense(128 , activation='relu'),\n",
    "    keras.layers.Dense(10)\n",
    "    #keras.layers.softamx as we need a probabilities for the 10 classes \n",
    "    #but it is preffered to be included in the loss phase\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7bad30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Dense in module keras.src.layers.core.dense:\n",
      "\n",
      "class Dense(keras.src.layers.layer.Layer)\n",
      " |  Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, lora_rank=None, **kwargs)\n",
      " |\n",
      " |  Just your regular densely-connected NN layer.\n",
      " |\n",
      " |  `Dense` implements the operation:\n",
      " |  `output = activation(dot(input, kernel) + bias)`\n",
      " |  where `activation` is the element-wise activation function\n",
      " |  passed as the `activation` argument, `kernel` is a weights matrix\n",
      " |  created by the layer, and `bias` is a bias vector created by the layer\n",
      " |  (only applicable if `use_bias` is `True`).\n",
      " |\n",
      " |  Note: If the input to the layer has a rank greater than 2, `Dense`\n",
      " |  computes the dot product between the `inputs` and the `kernel` along the\n",
      " |  last axis of the `inputs` and axis 0 of the `kernel` (using `tf.tensordot`).\n",
      " |  For example, if input has dimensions `(batch_size, d0, d1)`, then we create\n",
      " |  a `kernel` with shape `(d1, units)`, and the `kernel` operates along axis 2\n",
      " |  of the `input`, on every sub-tensor of shape `(1, 1, d1)` (there are\n",
      " |  `batch_size * d0` such sub-tensors). The output in this case will have\n",
      " |  shape `(batch_size, d0, units)`.\n",
      " |\n",
      " |  Args:\n",
      " |      units: Positive integer, dimensionality of the output space.\n",
      " |      activation: Activation function to use.\n",
      " |          If you don't specify anything, no activation is applied\n",
      " |          (ie. \"linear\" activation: `a(x) = x`).\n",
      " |      use_bias: Boolean, whether the layer uses a bias vector.\n",
      " |      kernel_initializer: Initializer for the `kernel` weights matrix.\n",
      " |      bias_initializer: Initializer for the bias vector.\n",
      " |      kernel_regularizer: Regularizer function applied to\n",
      " |          the `kernel` weights matrix.\n",
      " |      bias_regularizer: Regularizer function applied to the bias vector.\n",
      " |      activity_regularizer: Regularizer function applied to\n",
      " |          the output of the layer (its \"activation\").\n",
      " |      kernel_constraint: Constraint function applied to\n",
      " |          the `kernel` weights matrix.\n",
      " |      bias_constraint: Constraint function applied to the bias vector.\n",
      " |      lora_rank: Optional integer. If set, the layer's forward pass\n",
      " |          will implement LoRA (Low-Rank Adaptation)\n",
      " |          with the provided rank. LoRA sets the layer's kernel\n",
      " |          to non-trainable and replaces it with a delta over the\n",
      " |          original kernel, obtained via multiplying two lower-rank\n",
      " |          trainable matrices. This can be useful to reduce the\n",
      " |          computation cost of fine-tuning large dense layers.\n",
      " |          You can also enable LoRA on an existing\n",
      " |          `Dense` layer by calling `layer.enable_lora(rank)`.\n",
      " |\n",
      " |  Input shape:\n",
      " |      N-D tensor with shape: `(batch_size, ..., input_dim)`.\n",
      " |      The most common situation would be\n",
      " |      a 2D input with shape `(batch_size, input_dim)`.\n",
      " |\n",
      " |  Output shape:\n",
      " |      N-D tensor with shape: `(batch_size, ..., units)`.\n",
      " |      For instance, for a 2D input with shape `(batch_size, input_dim)`,\n",
      " |      the output would have shape `(batch_size, units)`.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      Dense\n",
      " |      keras.src.layers.layer.Layer\n",
      " |      keras.src.backend.tensorflow.layer.TFLayer\n",
      " |      keras.src.backend.tensorflow.trackable.KerasAutoTrackable\n",
      " |      tensorflow.python.trackable.autotrackable.AutoTrackable\n",
      " |      tensorflow.python.trackable.base.Trackable\n",
      " |      keras.src.ops.operation.Operation\n",
      " |      keras.src.saving.keras_saveable.KerasSaveable\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, lora_rank=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  build(self, input_shape)\n",
      " |\n",
      " |  call(self, inputs, training=None)\n",
      " |\n",
      " |  compute_output_shape(self, input_shape)\n",
      " |\n",
      " |  enable_lora(self, rank, a_initializer='he_uniform', b_initializer='zeros')\n",
      " |\n",
      " |  get_config(self)\n",
      " |      Returns the config of the object.\n",
      " |\n",
      " |      An object config is a Python dictionary (serializable)\n",
      " |      containing the information needed to re-instantiate it.\n",
      " |\n",
      " |  load_own_variables(self, store)\n",
      " |      Loads the state of the layer.\n",
      " |\n",
      " |      You can override this method to take full control of how the state of\n",
      " |      the layer is loaded upon calling `keras.models.load_model()`.\n",
      " |\n",
      " |      Args:\n",
      " |          store: Dict from which the state of the model will be loaded.\n",
      " |\n",
      " |  quantize(self, mode, type_check=True)\n",
      " |\n",
      " |  quantized_build(self, input_shape, mode)\n",
      " |\n",
      " |  save_own_variables(self, store)\n",
      " |      Saves the state of the layer.\n",
      " |\n",
      " |      You can override this method to take full control of how the state of\n",
      " |      the layer is saved upon calling `model.save()`.\n",
      " |\n",
      " |      Args:\n",
      " |          store: Dict where the state of the model will be saved.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |\n",
      " |  kernel\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.layers.layer.Layer:\n",
      " |\n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Call self as a function.\n",
      " |\n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |\n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |\n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |\n",
      " |  add_loss(self, loss)\n",
      " |      Can be called inside of the `call()` method to add a scalar loss.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```python\n",
      " |      class MyLayer(Layer):\n",
      " |          ...\n",
      " |          def call(self, x):\n",
      " |              self.add_loss(ops.sum(x))\n",
      " |              return x\n",
      " |      ```\n",
      " |\n",
      " |  add_metric(self, *args, **kwargs)\n",
      " |\n",
      " |  add_variable(self, shape, initializer, dtype=None, trainable=True, autocast=True, regularizer=None, constraint=None, name=None)\n",
      " |      Add a weight variable to the layer.\n",
      " |\n",
      " |      Alias of `add_weight()`.\n",
      " |\n",
      " |  add_weight(self, shape=None, initializer=None, dtype=None, trainable=True, autocast=True, regularizer=None, constraint=None, aggregation='mean', name=None)\n",
      " |      Add a weight variable to the layer.\n",
      " |\n",
      " |      Args:\n",
      " |          shape: Shape tuple for the variable. Must be fully-defined\n",
      " |              (no `None` entries). Defaults to `()` (scalar) if unspecified.\n",
      " |          initializer: Initializer object to use to populate the initial\n",
      " |              variable value, or string name of a built-in initializer\n",
      " |              (e.g. `\"random_normal\"`). If unspecified, defaults to\n",
      " |              `\"glorot_uniform\"` for floating-point variables and to `\"zeros\"`\n",
      " |              for all other types (e.g. int, bool).\n",
      " |          dtype: Dtype of the variable to create, e.g. `\"float32\"`. If\n",
      " |              unspecified, defaults to the layer's variable dtype\n",
      " |              (which itself defaults to `\"float32\"` if unspecified).\n",
      " |          trainable: Boolean, whether the variable should be trainable via\n",
      " |              backprop or whether its updates are managed manually. Defaults\n",
      " |              to `True`.\n",
      " |          autocast: Boolean, whether to autocast layers variables when\n",
      " |              accessing them. Defaults to `True`.\n",
      " |          regularizer: Regularizer object to call to apply penalty on the\n",
      " |              weight. These penalties are summed into the loss function\n",
      " |              during optimization. Defaults to `None`.\n",
      " |          constraint: Contrainst object to call on the variable after any\n",
      " |              optimizer update, or string name of a built-in constraint.\n",
      " |              Defaults to `None`.\n",
      " |          aggregation: String, one of `'mean'`, `'sum'`,\n",
      " |              `'only_first_replica'`. Annotates the variable with the type\n",
      " |              of multi-replica aggregation to be used for this variable\n",
      " |              when writing custom data parallel training loops.\n",
      " |          name: String name of the variable. Useful for debugging purposes.\n",
      " |\n",
      " |  build_from_config(self, config)\n",
      " |      Builds the layer's states with the supplied config dict.\n",
      " |\n",
      " |      By default, this method calls the `build(config[\"input_shape\"])` method,\n",
      " |      which creates weights based on the layer's input shape in the supplied\n",
      " |      config. If your config contains other information needed to load the\n",
      " |      layer's state, you should override this method.\n",
      " |\n",
      " |      Args:\n",
      " |          config: Dict containing the input shape associated with this layer.\n",
      " |\n",
      " |  compute_mask(self, inputs, previous_mask)\n",
      " |\n",
      " |  compute_output_spec(self, *args, **kwargs)\n",
      " |\n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |\n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |\n",
      " |  get_build_config(self)\n",
      " |      Returns a dictionary with the layer's input shape.\n",
      " |\n",
      " |      This method returns a config dict that can be used by\n",
      " |      `build_from_config(config)` to create all states (e.g. Variables and\n",
      " |      Lookup tables) needed by the layer.\n",
      " |\n",
      " |      By default, the config only contains the input shape that the layer\n",
      " |      was built with. If you're writing a custom layer that creates state in\n",
      " |      an unusual way, you should override this method to make sure this state\n",
      " |      is already created when Keras attempts to load its value upon model\n",
      " |      loading.\n",
      " |\n",
      " |      Returns:\n",
      " |          A dict containing the input shape associated with the layer.\n",
      " |\n",
      " |  get_weights(self)\n",
      " |      Return the values of `layer.weights` as a list of NumPy arrays.\n",
      " |\n",
      " |  quantized_call(self, *args, **kwargs)\n",
      " |\n",
      " |  set_weights(self, weights)\n",
      " |      Sets the values of `layer.weights` from a list of NumPy arrays.\n",
      " |\n",
      " |  stateless_call(self, trainable_variables, non_trainable_variables, *args, return_losses=False, **kwargs)\n",
      " |      Call the layer without any side effects.\n",
      " |\n",
      " |      Args:\n",
      " |          trainable_variables: List of trainable variables of the model.\n",
      " |          non_trainable_variables: List of non-trainable variables of the\n",
      " |              model.\n",
      " |          *args: Positional arguments to be passed to `call()`.\n",
      " |          return_losses: If `True`, `stateless_call()` will return the list of\n",
      " |              losses created during `call()` as part of its return values.\n",
      " |          **kwargs: Keyword arguments to be passed to `call()`.\n",
      " |\n",
      " |      Returns:\n",
      " |          A tuple. By default, returns `(outputs, non_trainable_variables)`.\n",
      " |              If `return_losses = True`, then returns\n",
      " |              `(outputs, non_trainable_variables, losses)`.\n",
      " |\n",
      " |      Note: `non_trainable_variables` include not only non-trainable weights\n",
      " |      such as `BatchNormalization` statistics, but also RNG seed state\n",
      " |      (if there are any random operations part of the layer, such as dropout),\n",
      " |      and `Metric` state (if there are any metrics attached to the layer).\n",
      " |      These are all elements of state of the layer.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```python\n",
      " |      model = ...\n",
      " |      data = ...\n",
      " |      trainable_variables = model.trainable_variables\n",
      " |      non_trainable_variables = model.non_trainable_variables\n",
      " |      # Call the model with zero side effects\n",
      " |      outputs, non_trainable_variables = model.stateless_call(\n",
      " |          trainable_variables,\n",
      " |          non_trainable_variables,\n",
      " |          data,\n",
      " |      )\n",
      " |      # Attach the updated state to the model\n",
      " |      # (until you do this, the model is still in its pre-call state).\n",
      " |      for ref_var, value in zip(\n",
      " |          model.non_trainable_variables, non_trainable_variables\n",
      " |      ):\n",
      " |          ref_var.assign(value)\n",
      " |      ```\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from keras.src.layers.layer.Layer:\n",
      " |\n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.src.layers.layer.Layer:\n",
      " |\n",
      " |  compute_dtype\n",
      " |      The dtype of the computations performed by the layer.\n",
      " |\n",
      " |  dtype\n",
      " |      Alias of `layer.variable_dtype`.\n",
      " |\n",
      " |  input_dtype\n",
      " |      The dtype layer inputs should be converted to.\n",
      " |\n",
      " |  losses\n",
      " |      List of scalar losses from `add_loss`, regularizers and sublayers.\n",
      " |\n",
      " |  metrics\n",
      " |      List of all metrics.\n",
      " |\n",
      " |  metrics_variables\n",
      " |      List of all metric variables.\n",
      " |\n",
      " |  non_trainable_variables\n",
      " |      List of all non-trainable layer state.\n",
      " |\n",
      " |      This extends `layer.non_trainable_weights` to include all state used by\n",
      " |      the layer including state for metrics and `SeedGenerator`s.\n",
      " |\n",
      " |  non_trainable_weights\n",
      " |      List of all non-trainable weight variables of the layer.\n",
      " |\n",
      " |      These are the weights that should not be updated by the optimizer during\n",
      " |      training. Unlike, `layer.non_trainable_variables` this excludes metric\n",
      " |      state and random seeds.\n",
      " |\n",
      " |  path\n",
      " |      The path of the layer.\n",
      " |\n",
      " |      If the layer has not been built yet, it will be `None`.\n",
      " |\n",
      " |  quantization_mode\n",
      " |      The quantization mode of this layer, `None` if not quantized.\n",
      " |\n",
      " |  trainable_variables\n",
      " |      List of all trainable layer state.\n",
      " |\n",
      " |      This is equivalent to `layer.trainable_weights`.\n",
      " |\n",
      " |  trainable_weights\n",
      " |      List of all trainable weight variables of the layer.\n",
      " |\n",
      " |      These are the weights that get updated by the optimizer during training.\n",
      " |\n",
      " |  variable_dtype\n",
      " |      The dtype of the state (weights) of the layer.\n",
      " |\n",
      " |  variables\n",
      " |      List of all layer state, including random seeds.\n",
      " |\n",
      " |      This extends `layer.weights` to include all state used by the layer\n",
      " |      including `SeedGenerator`s.\n",
      " |\n",
      " |      Note that metrics variables are not included here, use\n",
      " |      `metrics_variables` to visit all the metric variables.\n",
      " |\n",
      " |  weights\n",
      " |      List of all weight variables of the layer.\n",
      " |\n",
      " |      Unlike, `layer.variables` this excludes metric state and random seeds.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.src.layers.layer.Layer:\n",
      " |\n",
      " |  dtype_policy\n",
      " |\n",
      " |  input_spec\n",
      " |\n",
      " |  supports_masking\n",
      " |      Whether this layer supports computing a mask using `compute_mask`.\n",
      " |\n",
      " |  trainable\n",
      " |      Settable boolean, whether this layer should be trainable or not.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.trackable.base.Trackable:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.ops.operation.Operation:\n",
      " |\n",
      " |  symbolic_call(self, *args, **kwargs)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from keras.src.ops.operation.Operation:\n",
      " |\n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates an operation from its config.\n",
      " |\n",
      " |      This method is the reverse of `get_config`, capable of instantiating the\n",
      " |      same operation from the config dictionary.\n",
      " |\n",
      " |      Note: If you override this method, you might receive a serialized dtype\n",
      " |      config, which is a `dict`. You can deserialize it as follows:\n",
      " |\n",
      " |      ```python\n",
      " |      if \"dtype\" in config and isinstance(config[\"dtype\"], dict):\n",
      " |          policy = dtype_policies.deserialize(config[\"dtype\"])\n",
      " |      ```\n",
      " |\n",
      " |      Args:\n",
      " |          config: A Python dictionary, typically the output of `get_config`.\n",
      " |\n",
      " |      Returns:\n",
      " |          An operation instance.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.src.ops.operation.Operation:\n",
      " |\n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a symbolic operation.\n",
      " |\n",
      " |      Only returns the tensor(s) corresponding to the *first time*\n",
      " |      the operation was called.\n",
      " |\n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |\n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |\n",
      " |      Only returns the tensor(s) corresponding to the *first time*\n",
      " |      the operation was called.\n",
      " |\n",
      " |      Returns:\n",
      " |          Output tensor or list of output tensors.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.saving.keras_saveable.KerasSaveable:\n",
      " |\n",
      " |  __reduce__(self)\n",
      " |      __reduce__ is used to customize the behavior of `pickle.pickle()`.\n",
      " |\n",
      " |      The method returns a tuple of two elements: a function, and a list of\n",
      " |      arguments to pass to that function.  In this case we just leverage the\n",
      " |      keras saving library.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(keras.layers.Dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afa5e7e",
   "metadata": {},
   "source": [
    "**Another way of sequential model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19a3dbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moham\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "#the adv is it enables you to make a model.summary() after each layer \n",
    "model2 = keras.Sequential()\n",
    "model2.add(keras.layers.Flatten(input_shape = (28 ,28)))\n",
    "#model2.summary()\n",
    "model2.add(keras.layers.Dense(128 , activation='relu'))\n",
    "#model2.summary()\n",
    "model2.add(keras.layers.Dense(10))\n",
    "#model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a1b073",
   "metadata": {},
   "source": [
    "**Loss and optemizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eaaa97e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the y = 0 or 1 or 2 or 3 lables = indecies -> sparsecategoricalcrossentropy()\n",
    "#if y = oneHot -> y = [1 for the class and zeros for the rest]\n",
    "# ex y = [1 ,0 , 0, 0 ,0] so -> CategoricalCrossEntropy()only\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "#from logites = True bec we didn't put the softmax act function in the output layer yet\n",
    "optimizerr = keras.optimizers.Adam(learning_rate= 0.01)\n",
    "metric = ['accuracy']\n",
    "model.compile(loss= loss , optimizer= optimizerr , metrics = metric)\n",
    "\n",
    "#what is the diff between :\n",
    "#1-.SparseCategoricalCrossentropy() used in the validation while training and before testing\n",
    "#2-.sparse_categorical_crossentropy()used in the evaluation after training and while testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db65fb43",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc993584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 - 3s - 3ms/step - accuracy: 0.9330 - loss: 0.2207\n",
      "Epoch 2/5\n",
      "938/938 - 2s - 2ms/step - accuracy: 0.9617 - loss: 0.1304\n",
      "Epoch 3/5\n",
      "938/938 - 2s - 2ms/step - accuracy: 0.9688 - loss: 0.1092\n",
      "Epoch 4/5\n",
      "938/938 - 2s - 2ms/step - accuracy: 0.9707 - loss: 0.1036\n",
      "Epoch 5/5\n",
      "938/938 - 2s - 2ms/step - accuracy: 0.9748 - loss: 0.0910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x18b758477a0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to get the best training divide the batches and detect the number of epoches \n",
    "batches = 64 \n",
    "no_epoches = 5\n",
    "model.fit(x_train , y_train , batch_size= batches , epochs=no_epoches , verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0032ec8c",
   "metadata": {},
   "source": [
    "**Evaluate the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c349e260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 - 0s - 1ms/step - accuracy: 0.9715 - loss: 0.1293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.12930181622505188, 0.9714999794960022]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test , y_test , batch_size=batches , verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aecd7e6",
   "metadata": {},
   "source": [
    "Remember we didn't make the softmax in the layers instead w put it while training and now we want to make **Predictions** so we need it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af4be6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[1.3053491e-18 1.2329013e-14 2.1332693e-12 2.3264775e-10 3.1730135e-16\n",
      " 5.7600934e-15 2.6237464e-31 1.0000000e+00 1.3117997e-16 2.4126529e-13], shape=(10,), dtype=float32)\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "probability_model = keras.models.Sequential([\n",
    "   model , #now it has all the layers in the original(first) model\n",
    "   keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "\n",
    "predictions = probability_model(x_test)\n",
    "pred0 = predictions[0]\n",
    "print (pred0)\n",
    "label0 = np.argmax(pred0)\n",
    "print (label0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
